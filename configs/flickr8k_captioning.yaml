task_name: "flickr8k_captioning"

model:
  # We'll confirm the exact Qwen-VL model name later when we add model code; this is a placeholder.
  name: "Qwen/Qwen3-VL-8B-Instruct"
  revision: "main"
  cache_dir: ".cache/huggingface"
  torch_dtype: "bfloat16"   # or "float16" depending on GPU
  device: "cuda"            # overridden to "cpu" when testing locally if needed

dataset:
  hf_name: "flickr8k"
  split_train: "train"
  split_val: "validation"
  split_test: "test"
  image_column: "image"
  text_column: "text"
  # For quick experiments / debugging
  max_train_samples: null    # null == use all
  max_val_samples: 1000
  max_test_samples: 1000
  num_workers: 4

training:
  output_dir: "outputs/flickr8k_qwen3vl_lora"
  num_train_epochs: 2
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  weight_decay: 0.01
  max_grad_norm: 1.0
  warmup_ratio: 0.03
  logging_steps: 50
  eval_steps: 500
  save_steps: 1000
  save_total_limit: 3
  seed: 42
  fp16: true
  bf16: false
  resume_from_checkpoint: null

peft:
  enabled: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

generation:
  max_new_tokens: 64
  temperature: 0.7
  top_p: 0.9
  num_beams: 1
  prompt_template: |
    <image>
    You are an image captioning model. Describe this image in one concise sentence.

wandb:
  enabled: true
  project: "vlm-training"
  entity: null        # or your username
  run_name: "qwen3vl_lora"
