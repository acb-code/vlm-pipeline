task_name: "flickr8k_captioning"

model:
  name: "Qwen/Qwen3-VL-8B-Instruct"
  revision: "main"
  cache_dir: ".cache/huggingface"
  torch_dtype: "bfloat16"   # or "float16" if bf16 causes issues
  device: "cuda"

dataset:
  # We actually use jxie/flickr8k in load_flickr8k()
  hf_name: "jxie/flickr8k"
  split_train: "train"
  split_val: "validation"
  split_test: "test"
  image_column: "image"
  text_column: "caption_0"   # not used directly, but more honest
  max_train_samples: 2000    # null == use all
  max_val_samples: 500
  max_test_samples: 300
  num_workers: 4

training:
  output_dir: "outputs/flickr8k_qwen3vl_lora"
  num_train_epochs: 1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  weight_decay: 0.01
  max_grad_norm: 1.0
  warmup_ratio: 0.03
  logging_steps: 50
  eval_steps: 100
  save_steps: 200
  save_total_limit: 3
  seed: 42
  fp16: false
  bf16: true
  resume_from_checkpoint: null

peft:
  enabled: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

generation:
  max_new_tokens: 64
  temperature: 0.7
  top_p: 0.9
  num_beams: 1
  # IMPORTANT: no <image> here; image is handled via {"type": "image"} in messages
  prompt_template: "Describe this image in one concise, informative sentence."

wandb:
  enabled: true
  project: "vlm-training"
  entity: null        # or your username
  run_name: "qwen3vl_lora"
